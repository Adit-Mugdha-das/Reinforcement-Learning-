# Reinforcement Learning in Continuous State Spaces 

This assignment implements **reinforcement learning** using function approximation to handle continuous state spaces and learn optimal policies.  
It is part of **Week 3 (Course 3: Unsupervised Learning, Recommenders, Reinforcement Learning)** from the **Machine Learning Specialization** by **Andrew Ng** on Coursera.

##  Description

In this lab, I applied **Q-learning** with **function approximation** techniques to estimate value functions and policies in a continuous environment.  
This assignment bridges traditional tabular RL methods with function-based approaches that are scalable to real-world problems.

### Key Concepts Covered:
- Q-learning for reinforcement learning
- Continuous state spaces handling
- Function approximation for Q-value estimation
- Policy evaluation and improvement
- Exploration vs exploitation trade-off
- Training and evaluation of learned policies

##  Files Included

- `reinforcement_learning_lab.ipynb`: Jupyter notebook containing the full RL implementation and experiments
- `lab_utils_rl.py`: Helper functions for environment simulation and value updates
- `environments/`: Contains custom environments for agent training and testing

> ⚠️ This repository contains only my own code and adheres to Coursera’s Honor Code.

##  Tools Used

- Python 3
- NumPy
- Matplotlib
- Jupyter Notebook

##  Course Info

This lab is part of:
> [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)  
> Instructor: **Andrew Ng**  
> Course 3: Unsupervised Learning, Recommenders, Reinforcement Learning  
> Week 3: Reinforcement Learning in Continuous Spaces

##  License

This repository is for educational and portfolio purposes only. Please do not use it for direct assignment submission.

---

 Feel free to fork or star this repo if you're passionate about reinforcement learning and AI!
